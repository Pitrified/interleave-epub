{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a single ep and show some translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from transformers.pipelines import pipeline\n",
    "\n",
    "from interleave_epub.epub.epub import EPub\n",
    "from interleave_epub.nlp.cached_pipe import TranslationPipelineCache\n",
    "from interleave_epub.nlp.local_spacy_model import spacy_load_local_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_cache = Path(\"~/.cache/spacy_my_models\").expanduser()\n",
    "\n",
    "nlp = {\n",
    "    \"en\": spacy_load_local_model(\"en_core_web_md\", spacy_cache, force_download=False),\n",
    "    \"fr\": spacy_load_local_model(\"fr_core_news_md\", spacy_cache, force_download=False),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hug_cache = Path(\"~/.cache/hug_my_trad\").expanduser()\n",
    "hug_trad_en = hug_cache / \"translated_en-fr.json\"\n",
    "hug_trad_fr = hug_cache / \"translated_fr-en.json\"\n",
    "\n",
    "# pipe_fren = pipeline(\"translation\", model=f\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "pipe_fren = None\n",
    "\n",
    "pipe = {\n",
    "    \"en-fr\": TranslationPipelineCache(None, hug_trad_en, \"en-fr\"),\n",
    "    \"fr-en\": TranslationPipelineCache(pipe_fren, hug_trad_fr, \"fr-en\"),\n",
    "}\n",
    "\n",
    "pipe[\"en-fr\"](\"Let's try this cool way to create a callable class.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sample epub, in French\n",
    "epub_folder_path = Path(\"~\").expanduser() / \"snippet\" / \"datasets\" / \"ebook\"\n",
    "an_epub_path = epub_folder_path / \"Gaston_Leroux_-_Le_Mystere_de_la_chambre_jaune.epub\"\n",
    "an_epub_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it and translate the sentences\n",
    "an_epub = EPub(an_epub_path, \"Chambre Jaune\", \"fr\", \"en\", nlp, pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a sample paragraph\n",
    "a_chap = an_epub.chapters[1]\n",
    "a_par = a_chap.paragraphs[0]\n",
    "a_par\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a_chap.paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a_chap.sents_text[\"orig\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i_p, i_s), sent in a_chap.enumerate_sents(\"orig\"):\n",
    "    print(f\"{i_p} {i_s} {sent}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immediately derail everything to pick better sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_par.par_doc.has_annotation(\"SENT_START\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_doc = a_par.par_doc.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_s, sents in enumerate(a_doc.sents):\n",
    "    print(f\"{i_s} {sents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/api/doc#retokenize\n",
    "# https://stackoverflow.com/questions/58294798/spacy-doc-merge-to-using-retokenizer\n",
    "# https://stackoverflow.com/questions/65083559/how-to-write-code-to-merge-punctuations-and-phrases-using-spacy\n",
    "\n",
    "start_it = 15\n",
    "for it, token in enumerate(a_doc[start_it:25]):\n",
    "    print(\n",
    "        f\"{it+start_it:3d} {token.text:17s}\"\n",
    "        f\"{token.is_sent_start} {token.is_sent_end}\"\n",
    "    )\n",
    "\n",
    "    # does this work? No, of course\n",
    "    # if token.is_sent_start: token.is_sent_start = False\n",
    "    # Refusing to write to token.sent_start if its document is parsed, because this may cause inconsistent state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_doc[21 : 23 + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with a_doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(a_doc[21 : 22 + 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the sentences were actually merged\n",
    "a_doc[21].is_sent_start, a_doc[21].is_sent_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it again\n",
    "with a_doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(a_doc[21 : 22 + 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the sentences were actually merged\n",
    "a_doc[21].is_sent_start, a_doc[21].is_sent_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_doc[21 : 23 + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_s, sents in enumerate(a_doc.sents):\n",
    "    print(f\"{i_s} {sents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_it = 15\n",
    "for it, token in enumerate(a_doc[start_it:25]):\n",
    "    print(\n",
    "        f\"{it+start_it:3d} {token.text:17s}\"\n",
    "        f\"{token.is_sent_start} {token.is_sent_end}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('interleave-epub-QmszUGV--py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6684783418da4196e33b5bc6e66a97fdf8da1216b5b0b17466c551d04cad5e9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
